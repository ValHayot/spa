\documentclass{IEEEtran}
\usepackage[hyphens]{url}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{colortbl} % for \rowcolor
\usepackage{ulem} % for \uwave
\usepackage{caption}
\usepackage{subcaption}
\usepackage[bookmarks=false]{hyperref}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=blue,filecolor=black,urlcolor=blue}
\usepackage{algorithm}
\usepackage{algorithmicx} % Doc is at http://tug.ctan.org/macros/latex/contrib/algorithmicx/algorithmicx.pdf
\usepackage{algpseudocode}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}
\algblock{Input}{EndInput}
\algnotext{EndInput}
\algblock{Output}{EndOutput}
\algnotext{EndOutput}
\newcommand{\Desc}[2]{\State \makebox[2em][l]{#1}#2}

\newcommand{\tristan}[1]{\color{red}\textbf{Note from Tristan}:
      #1 \color{black}}
\newcommand{\TG}[1]{\tristan{#1}}

\newcommand{\todo}[1]{\marginpar{\parbox{18mm}{\flushleft\tiny\color{red}\textbf{TODO}:
      #1}}}
\definecolor{headcolor}{gray}{0.9}

\begin{document}
\title{Evaluation of pilot scheduling strategies for Apache
Spark clusters on HPC}
\author{
    \IEEEauthorblockN{
        Val\'erie Hayot-Sasson and Tristan Glatard
    }
    \IEEEauthorblockA{}
}
\maketitle

\begin{abstract}
    Big Data is becoming prominent throughout many scientific fields and, as a
    result, scientific communities are seeking Big Data frameworks to 
    accelerate the processing of their increasingly data-intensive pipelines.
    However, while scientific communities typically rely on High-Performance 
    Computing (HPC) clusters for the parallelization of their pipelines, many 
    popular Big Data frameworks such as Hadoop and Spark were primarily designed
    to be executed on dedicated commodity infrastructures. As Big Data frameworks
    cannot leverage HPC schedulers directly, they must be executed on an overlay
    cluster atop HPC allocations. This is problematic as application resource
    requirements needed by the HPC scheduler may not be known by the user. 
    Pilot scheduling strategies have been developed to address the limitations 
    of traditional HPC batch job schedulers. Pilot schedulers, such as HTCondor and DIRAC, 
    decouple resource
    provisioning from task scheduling, thereby enabling efficient resource
    utilisation through dynamic scheduling. This paper evaluates the benefits 
    pilot-scheduling strategies over traditional batch submission
    on HPC clusters with overlay Apache Spark clusters. We evaluate the
    overall speedup brought on by employing pilot-scheduling strategies through
    the application of four increasing resource configurations. Overall, we find
    that there is little benefit to using pilot scheduling strategies, though it
    can bring 2x when system queuing times are very slow. However, these
    occurrences are rare. Generally pilots have approximately the same makespan
    as batch. Despite makespan differences being found to be mostly due queueing 
    times, pilots did not appear to have any advantage in this regard, potentially
    due to system scheduling policies. Regardless, pilots may still be useful 
    when application walltimes are underestimated. This remains to be investigated.

   % 
    %We then examine the robustness to master and driver failures, which may be frequent 
    %when underestimating pilot walltime. Spark built-in checkpointing is also 
    %investigated in relation to pilot expiration.
\end{abstract}

\section{Introduction}

With the increasing volume of data being collected and shared, Big Data has become
an important area of focus to many scientific domains. As is the case with neuroimaging,
datasets available publicly now reach up to Petabytes in size~\cite{UKBiobank, hcp}. Average workstations found in research labs would not be
able to support the processing of such large datasets, leaving researchers to look 
into cloud or High-Performance Computing~(HPC) solutions. Whereas both options are
suitable for the processing of scientific Big Data, HPC clusters remain the more 
sought after solution, potentially due to their cost-effectiveness.

Scientific workflows may be quite compute intensive, requiring more than a few
hours to complete (\cite{freesurfer, fmriprep, a paper which mentions duration}).
The coupling of large datasets with compute-intensive workflows may result in 
unreasonably long processing times, which restricts the amount of data 
utilized in scientific experiments. To reduce the effects of Big Data on processing
times, researchers have begun to move towards employing Big Data frameworks for their
analyses, such as Hadoop MapReduce~\cite{Hadoop},
Apache Spark~\cite{Spark} and Dask~\cite{Dask}. However, these frameworks were 
designed with dedicated commodity infrastructure in mind, and, with the exception
of Dask, do not support traditional HPC schedulers.

Within HPC clusters, many users share resources to process a variety of different
applications. These shared resources include storage (e.g. parallel file system) and 
compute node resources (memory, CPU, local disk space, etc.). In order for users to 
gain access to shared compute resources, HPC clusters typically employ schedulers for 
batch resource provisioning. Examples of such schedulers include PBS, SGE, Slurm and 
TORQUE~\cite{schedulers}. These schedulers require that the user specify the amount
of resources required and the duration that they require the resources for. The users 
are then placed in a queue at a position based on the user's priority and resource 
availability.

There are several limitations to running applications on batch HPC schedulers, 
particularly in the face of Big Data. For instance, users may not necessarily 
know the exact requirements of the applications. With applications that are 
both data- and compute-intensive, this may result in the user overestimating the
resources required and the duration that these resources are required for. Frequent
high overestimates may create bottlenecks in the system, preventing other users
from occupying allocated idle resources. In contrast, users may underestimate required 
resources, leading the batch scheduler to kill applications due to exceeded resource quotas.
This may subsequently result in many failures within the application leading to
additional re-executions of the same application. This is problematic for the user
as their priority may be affected by the cluster usage. Additionally, the user may 
require more time on the cluster to relaunch non-checkpointed and interrupted 
tasks. \uwave{Furthermore, Big Data frameworks, such as Apache Spark, do not interact naturally
with HPC schedulers and typically rely on other schedulers for resource provisioning
and task scheduling. Therefore, to run Big Data applications on HPC schedulers, 
it is also necessary to start an overlay cluster.} \tristan{The previous 2 sentences are a bit unclear. You should explain
what an overlay cluster means here. ``Interact with other schedulers'' is vague. This point should end up explaining
what static resource provisioning means for Spark, i.e., the Compute Canada approach. It might even be a separate paragraph.
You could merge here the paragraph that currently explains that later.}


In contrast to static resource provisioning, as is the case with batch HPC schedulers, 
dynamic resource provisioning, known as pilot job or glide-in scheduling, exists. 
Dynamic resource provisioning allows the user to specify a minimum and maximum 
amount of resources, and the cluster can expand and contract depending on application requirements.
\tristan{Add one sentence to define what's a pilot}. 
Each pilot may consist of the minimal amount of resources required to launch the 
application. As each pilot would request allocation of fewer resources than that of a batch
request, Pilot Jobs would typically spend less time on the resource allocation 
queue. Furthermore, although individual pilots may expire due to walltimes, the dynamic
nature of pilots allows the entire application to surpass walltime limitations, as
additional pilots may be added to the pilot queue until application completion. 
Examples of HPC schedulers employing pilot strategies include HTCondor~\cite{htcondor}
and DIRAC~\cite{DIRAC}. \todo{maybe mention PSOM and Pegasus here \tristan{Yes for PSOM as it's neuroimaging. Pegasus
only uses pilots through condor afaik.}}

\tristan{I think this could be the 3rd paragraph.} Apache Spark is a popular Big Data framework, commonly used in both industrial
and academic settings. Although it is a Scala-based framework, it also has APIs
for Java, Python (PySpark) and R. Spark's Resilient Distributed Dataset (RDD) abstraction enabled
in-memory processing of pipelines by co-locating tasks and data, which provided important performance improvements compared to its predecessors.
 Through the use of RDDs, it also became possible to execute iterative 
workflows -- something not easily doable in older frameworks such as Hadoop MapReduce.
Schedulers for Spark include its built-in standalone schedule, Yet Another Resource Negotiator (YARN),
and Mesos~\cite{yarn, mesos}. \tristan{You could cite Ariel's and your paper to say that Spark is sometimes useful in 
neuroimaging on a cluster.}


Projects such as Thunder~\cite{thunder}, a PySpark-compatible library 
of image processing tools, have been developed in various fields 
although they are still not widely adopted, possibly due to the 
difficulties in adapting Big Data frameworks on HPC \todo{better 
explain the difficulties...setting walltimes etc. also explain 
neuroimaging 
applications}. In addition, Spark has been used in 
various research projects to process neuroimaging data \cite{Boubela, 
ariel's paper, maybe the simulation one}. \tristan{in this paragraph you could refer
to your pre-print for more information.} \tristan{I would summarize that in 1 or 2 sentences at the end of the previous
paragraph.}

The recommended method for launching Spark applications on HPC schedulers 
involves batch requesting all the necessary resources and launching a standalone
Spark cluster once the resources have been requested \TG{Add link to Compute Canada's instructions?}. This could significantly increase
the scheduling time of Spark-based applications as a large amount of resources 
may be necessary to process Big Data. In a pilot-scheduling model, rather than requesting all the resources 
at once, a Spark cluster is launched with a subset of the resources and is expanded as 
more resources get allocated. This, in turn, may reduce the overall processing of
an application as less resources are requested at once and are therefore more likely
to be scheduled faster.

Pilot scheduling techniques have already been applied to scientific workflow engines, such as
Pegasus, PSOM and RADICAL-Pilot~\cite{}. While there have been some efforts on
implementing pilot scheduling approaches for Apache Spark\cite{jha and spark on pbs paper},
research is limited and none of the them detail the effects of an overlay cluster
on overall performance and robustness. 
\tristan{This paragraph should write the scientific question of your paper more clearly: is it useful to 
use a pilot job framework to run Spark workflows on HPC? Also, this comes a bit late in the intro, it would be good
to precise the question early on, and the explain it in length as you do.}

In this paper, we will analyse the effects of pilot scheduling strategies with an 
overlay Spark cluster. Our evaluation will be broken down into two parts: 1)
the added value of pilots, and 2), tolerance to failures. For determining the 
added value of pilots, we will investigate the effect of queuing times on 
pilot applications as compared to a batch requests. As much of the application's
tolerance to failures are features brought on by the overlay Spark cluster, in
Part 2 we will introduce failures that would be typical to pilot scheduling 
approaches (e.g. node failures due to walltime expiration). This part will be 
broken down into master failures, driver failures and checkpointing. As pilots 
may need to be replenished during execution, we will also consider the effects of 
job queues \todo{reword}.

\tristan{Your text now mentions points 3 and 4 below, but doesn't talk about 1 and 2 and the fact that
an overlay cluster already deals with that. Something that wasn't mentioned in my initial list was that pilot jobs
also reduce the number of tasks seen by the cluster (this is also addressed by an overlay scheduler). It would be interesting
to explain that many of the interests of pilot jobs are already addressed by overlay scheduling, hence the question whether
pilot jobs are useful at all in this context.}
\TG{The introduction is still very much geared toward the presentation of 
the SPA software. Instead, I think it should introduce the question whether pilot jobs
are useful at all to overlay schedulers, explain why they may be, and why they may not (there is already an overlay scheduler,
and resource heterogeneity is limited). You could explain that the traditional benefits of pilot-job systems are:
\begin{itemize}
    \item Address worker node heterogeneity
    \item Address worker node failures (software, incl walltime expirations, or hardware)
    \item Address variable queuing times
    \item Tolerant to resource requirement overestimation, in particular walltime and CPU
    \item Increase workload distribution (use more nodes)
\end{itemize}
Among these, (1) and to some extent (2) are already addressed by an overlay scheduler.

The title should also reflect this.
}

\section{Materials and Methods}\label{sec:methods}

    The application, templates, configuration files, benchmarks and
    analysis scripts are publicly available and can be found in our Spark
    Pilot-job scheduler for HPC Applications (SPA) software at:
    \href{https://github.com/big-data-lab-team/spa}{https://github.com/big-data-lab-team/spa}.
    Links to the processing engines and processed data are provided in the
    text.
    
    \subsection{Infrastructure}
    All experiments were conducted on the Cedar and B\'eluga HPC computing clusters
    made available by Compute Canada through WestGrid and Calcul Qu\'ebec \TG{add links}. Both 
    clusters are accessible through the Slurm batch scheduler and Lustre parallel file system.
    The Cedar cluster has a total of 1542 nodes
    with a total of 58,416 CPU cores. Available memory on a Cedar node can range
    from 125 to 3022~G. Standard nodes are equipped with either 2x Intel E5-2683 v4
    Broadwell @ 2.1~Ghz (32 cores total) or 2x Intel Platinum 8160F Skylake @
    2.1Ghz (48 cores total) CPUs and 
    2 x 480~G SSD. All nodes and temporary storage on Cedar are connected by an 
    Intel OmniPath (version 1) with 100Gbit/s bandwidth.

    B\'eluga, on the other hand, is a smaller cluster with 872 available nodes. 
    Node memory can range between 92 to 752~G, with the most common node type having 
    186G. All nodes contain 2 x Intel Gold 6148
    Skylake @ 2.4~Ghz (40 cores/node) CPU and are connected to each other with a
    100~Gb/s Mellanox Infiniband EDR network \TG{expand EDR}. Each non-GPU node type contains one 
    480~G SSD. 

    \subsection{Job Templates}

    Two different job templates were used to implement the two main
    conditions compared in our experiments: the batch submission
    template and the pilot submission template. The batch submission template was
    inspired by the template provided by Compute Canada to launch Spark applications 
    on Slurm \href{https://docs.computecanada.ca/wiki/Apache\_Spark/en}{available here}.
    The template operates as follows: Certain resource requirements are requested by
    the user (e.g. walltime, amount of memory per node, number of CPUs per task, number 
    of nodes and number of tasks per node). Once these resources are allocated, a 
    master is started on one of the requested node resources. Then, after the master
    has successfully started, the workers are started on all nodes. Multiple worker
    instances are started on a single node by setting the \texttt{SPARK\_WORKER\_INSTANCES}
    environment variable to the number of tasks per node. Each worker is given as many
     cores as specified by the user in the Slurm resource allocation request.
    After both the masters and the workers have successfully started, the driver is finally
    started. The amount of memory given to each executor corresponds to 95\% of the 
    available memory on the node, to allow for offheap space. 
    The Spark deploy-mode selected for the batch template is client-mode. 

    The pilot submission template is similar to that of the batch template, however,
    each pilot will start its own Spark master and worker. Only one pilot,
    the first pilot to attempt to launch the driver, will do so. The reason for 
    which each pilot starts its own master is to ensure the fault tolerance of the 
    masters. In this sort of configuration, should the active master be killed, one
    of the stand-by masters will takeover and the application will be able to 
    resume. Such a configuration is particularly favourable in pilot scheduling 
    scenarios as node failures may be more frequent due to walltime expiration.
    Additionally, the Spark deploy mode of the driver was selected to be cluster deploy mode.
    This would not only allow the driver to be executed directly on one of the workers,
    but also allow us to make the driver fault-tolerant through the
    \texttt{supervise} mode, which is only available in cluster deploy
    \TG{A brief section on Spark to explain these modes and features would
    be useful}. As with the masters, it is particularly important to have a
    fault-tolerant driver in pilot-scheduling scenarios due to possible
    walltime expiration \TG{It should be explained before that one of the interests of pilots
    is to ``play'' with shorter walltimes}. \TG{It would also be nice to have runs where pilots 
    have shorter walltimes, to see if queuing times are shorter too.}
    Should pilots be idle for a certain duration, the
    pilots will terminate themselves such as to not hog resources.
    
    Due to the differences in deploy modes between batch and pilot submission, 
    batch will always inevitably have one more worker than pilot. This is because
    in cluster deploy, which pilot uses, the driver occupies a worker, whereas in
    client deploy, the driver is separate from any worker.

    Both of these Slurm templates are launched within a Python application called
    \textit{SPA}. The templates are used in conjunction with JSON configuration 
    file and passed to the SlurmPy~\cite{slurmpy} library within \textit{SPA}.
    The \textit{SPA} application, all the while ensures that all is preconfigured
    correctly before passing it to SlurmPy. It also ensures that enough pilots are
    launched, maintains track of the running/queued pilots, and launches additional
    pilots if there are less pilots than requested by the user in the Slurm queue.



    \subsection{Application}
        \begin{algorithm}\caption{Incrementation}\label{alg:incrementation}              
            \begin{algorithmic}[1]                                                       
                \Input                                                                       
                    \Desc{$x$}{a sleep delay in seconds}                                         
                    \Desc{$n$}{a number of iterations}                                           
                    \Desc{$C$}{a set of image chunks}                                            
                \EndInput                                                                    
                \ForEach{$chunk \in C$}                                                      
                    \State read $chunk$                                        
                    \For{$i \in [1, n]$}                                                         
                        \State $chunk\gets chunk+1$                                              
                        \State sleep $x$ \TG{Don't you sleep by x minus the duration of the previous step?}                                                        
                    \EndFor                                                                      
                    \State write $chunk$                                            
                \EndFor                                                                      
            \end{algorithmic}                                                                
        \end{algorithm}
    To determine the added value of pilot scheduling over batch scheduling of Spark
    applications, we required a Spark application operating on a large dataset with
    an important processing time to emulate a what would be the average requirements
    of a scientific Spark application. For this, we created a synthetic application 
    that would process the Big Brain~\cite{bigBrain}, a 76~G 3D histological
    image of a human brain. The algorithm is a chain of map transformations that
    at each transformation increment the voxels of the image by 1 (see Algorithm~\ref{alg:incrementation}).
    We chose such a synthetic algorithm
    as the focus of our experiments is pilot scheduling and not the application in
    itself. Furthermore, this algorithm enabled us to have control over the task duration
    which was representative of scientific applications. Additionally, it was important
    that the overall application duration did not vary between the different levels of
    parallelism within our experiments. Being able to adjust the task duration based on
    level of parallelism allowed us to achieve this.

    Investigating the fault-tolerance provided by the overlay cluster is
    crucial in determining the suitability of pilot scheduling with overlay clusters.
    Fault-tolerance of the driver is only possible in cluster deploy mode, however,
    when using Spark's standalone scheduler, this mode is not available for Python 
    applications. It is for this reason that our synthetic application is written
    in Scala. Nevertheless, cluster deploy mode is possible for Python application
    using YARN or Mesos schedulers.

    
   
    \todo{walltime parameter of workflow}
    \subsection{Model}

    The makespan of an application can be defined as the total duration
    between the submission time of the first application task, and the
    completion time of the last application time. It includes any
    scheduling time, queueing time, data transfer time, and any other
    overhead.
    
    Assuming a divisible load, i.e., the application can be divided in any
    number of tasks, the makespan can be expressed using the following
    expression, which holds for both batch and pilot execution modes:
    \begin{equation}
        M = \frac{C}{W} \label{eq:mcw}
    \end{equation}
    where:
    \begin{itemize}
        \item $M$ is the makespan of the application
        \item $C$ is the total CPU time of the application
        \item $W$ is the average number of Spark workers throughout the execution
    \end{itemize}
    The average number of workers $W$ allows us to obtain a more accurate makespan
    figure as the average number of workers will decrease with increasing 
    queuing times. It is computed as follows:
    \begin{equation}
        W = \frac{1}{M}\int_0^M{w(t)dt}\label{eq:avgw}
    \end{equation}
    where $w(t)$ is the number of workers available at time $t$. When the
    application is not subject to any scheduling or queuing time, the
    average number of workers equal the number of workers requested. 

    Therefore, assuming a fixed total CPU time, the relation
    between batch and pilot jobs can be represented as:
    \begin{equation}
        \frac{M_{batch}}{M_{pilot}} = \frac{W_{pilot}}{W_{batch}}\label{eq:makespancomp}
    \end{equation}
    where:
    \begin{itemize}
        \item $M_{batch}$ is the makespan of the batch application
        \item $M_{pilot}$ is the makespan of the pilot application
        \item $W_{pilot}$ is the average number of workers of the pilot application
        \item $W_{batch}$ is the average number of workers of the batch application
    \end{itemize}
    We will use this relation to discuss our results later on. It
    corresponds to an ideal case where no data or other overhead is
    present: only queuing times are included.

    \todo{perhaps expand a bit here \TG{I have a formal proof for eq (1) but I don't think
    it's required here.}}

    \subsection{Added value of pilot scheduling}
        \begin{table}                                                                    
            \centering                                                                       
            \begin{tabular}{c|c|c}                                                             
            \rowcolor{headcolor}                                                             
            Configuration & RAM (GB) & Tasks \\                               
            \hline                                                                           
            1 & 112 & 16 \\                                               
            2 & 224 & 32 \\                                               
            3 & 336 & 48 \\
            4 & 448 & 64 \\
            \end{tabular}                                                                    
            \setlength{\belowcaptionskip}{-10pt}                                             
            \caption{Resource configurations}                                                    
            \label{table:dedicatednodes}                                                            
        \end{table} 
           
        \begin{table*}                                                                   
        \centering                                                                       
        \begin{tabular}{c|cccccc}                                                   
          \rowcolor{headcolor}                                                           
          \multicolumn{7}{c}{Configuration 1}\\                      
          \hline                                                                         
          \rowcolor{headcolor}                                                           
          execution mode & nodes/job & RAM (G) & CPUs per task & tasks/node & walltime & task delay (s) \\                             
          \hline
          Batch & 1 & 112 & 1 & 16 & 2h30 & 45 \\
          8 pilots & 1 & 14 & 1 & 2 & 2h30 & 45 \\
          16 pilots & 1 & 7 & 1 & 1 & 2h30 & 45 \\

          \hline                                                                           
          \multicolumn{7}{c}{}\\                                                        

          \rowcolor{headcolor}                                                           
          \multicolumn{7}{c}{Configuration 2}\\                      
          \hline                                                                         
          \rowcolor{headcolor}                                                           
          execution mode & nodes/job & RAM (G) & CPUs per task & tasks/node & walltime & task delay (s) \\                             
          \hline
          Batch & 2 & 112 & 1 & 16 & 2h30 & 90 \\
          8 pilots & 1 & 28 & 1 & 4 & 2h30 & 90 \\
          16 pilots & 1 & 14 & 1 & 2 & 2h30 & 90 \\

          \hline                                                                           
          \multicolumn{7}{c}{}\\                                                        

          \rowcolor{headcolor}                                                           
          \multicolumn{7}{c}{Configuration 3}\\                      
          \hline                                                                         
          \rowcolor{headcolor}                                                           
          execution mode & nodes/job & RAM (G) & CPUs per task & tasks/node & walltime & task delay (s) \\                             
          \hline
          Batch & 3 & 112 & 1 & 16 & 2h30 & 120 \\
          8 pilots & 1 & 42 & 1 & 6 & 2h30 & 120 \\
          16 pilots & 1 & 21 & 1 & 3 & 2h30 & 120 \\

          \hline                                                                           
          \multicolumn{7}{c}{}\\                                                        

          \rowcolor{headcolor}                                                           
          \multicolumn{7}{c}{Configuration 4}\\                      
          \hline                                                                         
          \rowcolor{headcolor}                                                           
          execution mode & nodes/job & RAM (G) & CPUs per task & tasks/node & walltime & task delay (s) \\                             
          \hline
          Batch & 4 & 112 & 1 & 16 & 2h30 & 180 \\
          8 pilots & 1 & 56 & 1 & 8 & 2h30 & 180 \\
          16 pilots & 1 & 28 & 1 & 4 & 2h30 & 180 \\

          \hline                                                                           
          \multicolumn{7}{c}{}\\                                                        
        \end{tabular}                                                                    
        \setlength{\belowcaptionskip}{-10pt}                                             
        \caption{Experimental conditions}
        \label{table:conditions}                                                        
        \end{table*}                                                                                   
        To determine if there are any performance benefits to using pilot over 
        batch scheduling, we needed to compare both strategies given various resource
        requirements. It is expected that large batch requests will stay in the 
        resource queue longer than multiple pilot requests that ultimately use up
        the same amount of resources as each individual pilot requests less resources at
        a time. We therefore used four resource configurations to investigate this
        hypothesis (Table~\ref{table:dedicatednodes}).

       For batch, we requested 1 to 4 dedicated nodes, depending on resource 
       configuration, each with 112~G of RAM, 1 core per task and 16 tasks per 
       node. On the other hand, for all configurations, we ran our experiments
       with 8 and 16 pilots. Experimental conditions can be seen in Table~\ref{table:conditions}.

       As the focus of our experiments here is to measure the impact pilot scheduling
       has on queueing times, we wanted to ensure that our walltime estimates would
       be consistent for each experimental condition. Therefore, we adjusted task 
       delay based on the maximum level of parallelism to ensure that walltimes would
       not need to be readjusted for each configuration. Given utmost parallelism,
       the sleep delay added would amount to 1 hour of processing time regardless
       of configuration. The real processing time of the application, however, 
       would differ between configurations as we did not account for task duration
       in the sleep delay. However, task duration was not expected to signicantly impact
       overall processing time, but nevertheless, we set application walltime to 
       2 hours and 30 minutes for all configurations to account for additional
       processing time that may be incurred in applications with lower levels of 
       parallelism.

       For each configuration, all three execution modes were executed in parallel
       to ensure that the status of the overall cluster was the same when the 
       different execution modes were launched. Furthermore, the order of the execution modes
       for each configuration was randomized to ensure that SLURM job request
       order could not have affected results. 

       There were 15 repetitions in total for each configuration, and the order
       in which the configurations were launched was also randomized. This was to 
       account for any system variability that can occur, particularly in production
       HPC clusters. 

       Different clusters may have different resource configurations, number of 
       users and scheduling policies. Therefore, we executed all 15 repetitions on
       both Cedar and B\'eluga to determine how much our results differ between
       two distinct clusters.

    % maybe remove
    \subsection{Pipeline robustness}
        While nodes failures can occur for various reasons, underestimation of 
        walltime may lead to early node termination in pilot scheduling systems.
        Therefore, it is crucial for the overlay cluster within the pilot system
        to be robust to node failures. In this series of experiments, we investigate
        Spark's fault-tolerance with respect to worker, master and driver termination.

        In our configuration, the driver is running in cluster deploy-mode, resulting
        in it being executed directly on a worker. This feature alone should not make
        it fault-tolerant to failures. However, using this feature coupled with the 
        \texttt{--supervise} flag, will. 



        

        \todo{scala standalone vs our pyspark workaround. kill masters in experiments}
    % maybe remove\subsection{Checkpointing}
        \todo{metric for determining how often to checkpoint based on cluster size}
    % maybe remove \subsection{Job arrays}
        \todo{need to kill idle workers. may not want all workers to be running at once.}
    % maybe remove \subsection{Example application}
        \todo{incrementation with varying task durations}
\section{Results}\label{sec:Results}

As can be seen in both figures~\ref{fig:makespansbeluga} and ~\ref{fig:makespanscedar}, pilots 
generally did not bring any performance improvement to the execution of Spark workloads
on Beluga and Cedar. In Beluga (Figure~\ref{fig:beluga1}), 16 pilots were always 
slower than batch except for repetitions 4 and 12. The maximum speedup achieved
was approximately 1.3~x for 8 pilots and 1.1~x for 16 pilots. In Figures~\ref{fig:beluga2}
and ~\ref{fig:beluga3}, it is also the case that batch is generally faster than
pilots for the majority of the cases, except a couple, in which the speedup brought
by use of pilots is very limited. It is only in Configuration~4 (Figure~\ref{fig:beluga4})
that we see a maximum speedup of 2.8, however, it generally remains that pilots are slower.

The Cedar cluster (Figure~\ref{fig:makespanscedar}) also follows the same trend that
the makespan of batch scheduled applications are generally faster than that of pilots,
with a maximum speedup of 3, found both in Configuration~1 and 4. Moreover, in 
Cedar, it was less clear whether to use 8 or 16 pilots as speedup between the two
varies between the iterations. In general, both pilot configurations performed the
same.

In order to determine whether queueing time was the cause of the difference 
in makespans between batch and pilots, we compared makespan with queuing time of 
first pilot. As can be seen in Figure~\ref{fig:mqcedar}, variations in overall
makespans is primiarily due to queueing time.

In the case of pilots, multiple Slurm jobs, each with potentially different queuing times,
are used. Figure~\ref{fig:mwall} displays the makespan times for an average number of workers
given an experiment configuration, where the line corresponds to the estimated makespan using 
our model. Makespan for the model was calculated as:
$$
M = (\lceil 125/W \rceil\times d) + 20,
$$
where:
\begin{itemize}
    \item $M$ is the makespan of the application
    \item $W$ is the average number of Spark workers throughout the execution
    \item $d$ is the task delay associated with a given configuration
    \item $20$ is the average measured incrementation duration for a BigBrain chunk
\end{itemize}
As can be seen in the figure, although pilots and batch had similar makespans
for a given number of average workers, pilots were consistently slower than batch
for the same number of workers. Both batch and pilots were consistent with the 
model, however. In general, both pilots and batch had the same number of average
workers.

    \begin{figure*}
        \centering
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/dedicated_1_beluga}
            \caption[]%
            {{\small Configuration 1}}
            \label{fig:beluga1}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/dedicated_2_beluga}
            \caption[]%
            {{\small Configuration 2}}
            \label{fig:beluga2}
        \end{subfigure}
        \vskip\baselineskip
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/dedicated_3_beluga}
            \caption[]%
            {{\small Configuration 3}}
            \label{fig:beluga3}
        \end{subfigure}
        \quad
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/dedicated_4_beluga}
            \caption[]%
            {{\small Configuration 4}}
            \label{fig:beluga4}
        \end{subfigure}
        \caption[]
        {\small The application makespan on the Beluga cluster for all repetitions.}
        \label{fig:makespansbeluga}
    \end{figure*}


    \begin{figure*}
        \centering
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/dedicated_1_cedar}
            \caption[]%
            {{\small Configuration 1}}
            \label{fig:cedar1}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/dedicated_2_cedar}
            \caption[]%
            {{\small Configuration 2}}
            \label{fig:cedar2}
        \end{subfigure}
        \vskip\baselineskip
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/dedicated_3_cedar}
            \caption[]%
            {{\small Configuration 3}}
            \label{fig:cedar3}
        \end{subfigure}
        \quad
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/dedicated_4_cedar}
            \caption[]%
            {{\small Configuration 4}}
            \label{fig:cedar4}
        \end{subfigure}
        \caption[]
        {\small The application makespan on the Cedar cluster for all repetitions.}
        \label{fig:makespanscedar}
    \end{figure*}

      \begin{figure*}
        \centering
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/dedicated_4_cedar}
            \caption[]%
            {{\small Makespan}}
            \label{fig:makespancedar}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/pq_4_cedar}
            \caption[]%
            {{\small Queuing time}}
            \label{fig:queuecedar}
        \end{subfigure}
        \caption[]
        {\small Side-by-side comparison of total makespan and queueuing time of
        first pilots or batch application for Configuration 4 running on Cedar}
        \label{fig:mqcedar}
    \end{figure*}


    \begin{figure*}
        \centering
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/mw_beluga}
            \caption[]%
            {{\small Beluga}}
            \label{fig:mwbeluga}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/mw_cedar}
            \caption[]%
            {{\small Cedar}}
            \label{fig:mwcedar}
        \end{subfigure}
        \caption[]
        {\small The relation between makespan and average number of workers as 
        calculated using Equation~\ref{eq:avgw}. Trendline denotes the expected makespan
        given the average number of workers given a certain configuration}
        \label{fig:mwall}
    \end{figure*}

\section{Discussion}\label{sec:discussion}

%SPA vs other frameworks.

%Technical overhead of using pilots + an overlay cluster over a cluster.
Pilots do not appear to provide any performance advantage over batch scheduling.
This can be due to a few reasons. The queuing time for batch and pilots were similar, 
which can be an indication that the required resources to run the batch requests were
available, and our priority on the cluster was what affected the queuing time. This may, in turn,
explain why batch was sometimes significantly slower than pilots -- queueing time was not
only controlled by priority, but also by available resources.

It was also found that having 16 pilots was generally slower than 8 for the same
configuration. This was particularly the case on Beluga. We suspect this might be
related to the number of Slurm jobs permitted for a user on the backfill queue. For
all configurations, we ran batch, 8 pilots and 16 pilots concurrently. This would have
resulted in 25 Slurm jobs trying the access the queue concurrently. On Beluga, this
feature is configured to a total of 10 jobs. For 16 nodes, this would at best be
only 62.5\% of the pilots being scheduled at a given time if all resources are available.
The next 6 jobs would then have to wait 3 minutes before having an opportunity to enter the
backfill. Cedar, on the other hand, permits 40 user jobs to be backfilled at a given time.
Therefore, on Cedar, all concurrent Slurm jobs had an opportunity to be backfilled at
the same time, which may explain why this behaviour is more apparent on B\'eluga. Furthermore,
while both pilot configurations could be places in B\'eluga's low memory nodes, batch requests
had to be place on B\'eluga's medium memory nodes. B\'eluga has 172 low memory nodes and
516 mid memory nodes. As pilots would get priority for the low memory nodes first and 
low memory nodes are less frequent that medium memory nodes, it is possible that this
may have increased the overall queuing times of the pilots. However, Cedar's basic
node has enough memory to for all batch and pilot configuration, therefore, this could not
have affected queuing time for Cedar.

Although queuing time is largely responsible for makespan variations, it is not
always entirely responsible for the difference. Sometimes there are errors related
to workers not registering properly with the master. These occurences lead to the 
pipeline being processed entirely with a smaller number of available workers. As the task
delay added considers the maximum amount of parallel workers, functioning with less total workers
will significantly affect makespan. However, as seen in Figure~\ref{fig:mwall}
pilots are slower even with the same amount of average workers. A potential reasoning for this
may be the, worker registration delay and the time required to transfer data
over after the application has already started. In batch scheduling, all workers a started in
parallel and the data is transferred automatically at the beginning of the pipeline as the maximum amount
of parallelism is available at driver start time. Pilots, on the other hand, may start workers in parallel,
but not necessarily all will be started at the same time. This means that the entire 
workflow may suffer the impacts of starting workers in sequence. Moreover, there would also
be a delay with respect to transferring data from workers to newly added workers. Therefore,
it is only natural that pilots would have a bit more overhead that batch given the same
queuing time should pilots not all commence at once. Furthermore, the pilot scripts also
have some starter overhead as the all start a master, a worker (registered to the main
master) and attempt to start a driver. Batch need only start a single master and attempts to start
a driver only once.

The motivation to use pilots, however, is dynamic scheduling on HPC clusters. While we
investigated the queuing time differences with respect to differences in available resources,
the walltimes were kept static. Had walltime been underestimated to the point where the Slurm jobs
would terminate prior to application completion, pilots would have been preferable to batch
as batch scheduling has no mechanism to restart. Even if such a mechanism was in place, the entire
cluster would be shutdown and need to restart, which has additional overheads. With pilots, as long as 
there was at least one pilot alive, it would be possible to maintain the cluster and add additional pilots.
Spark provides built-it fault-tolerance for not only the workers, but the master and driver as well.



\section{Conclusion}\label{sec:conclusion}

\end{document}
