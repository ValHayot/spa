\documentclass{IEEEtran}
\usepackage[hyphens]{url}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{colortbl} % for \rowcolor
\usepackage{caption}
\usepackage{subcaption}
\usepackage[bookmarks=false]{hyperref}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=blue,filecolor=black,urlcolor=blue}

\newcommand{\tristan}[1]{\color{red}\textbf{Note from Tristan}:
      #1 \color{black}}

\newcommand{\todo}[1]{\marginpar{\parbox{18mm}{\flushleft\tiny\color{red}\textbf{TODO}:
      #1}}}
\definecolor{headcolor}{gray}{0.9}

\begin{document}
\title{SPA: An Apache Spark Pilot scheduler\\ for HPC applications}
\author{
    \IEEEauthorblockN{
        Val\'erie Hayot-Sasson and Tristan Glatard
    }
    \IEEEauthorblockA{}
}
\maketitle

\begin{abstract}
    Big Data processing engines such as Apache Spark are increasingly 
    used to process scientific data. These processing engines, however, 
    have been primarily designed to work on dedicated infrastructure 
    and are not well designed to work on High-Performance Computing 
    (HPC) systems commonly available to researchers. Typical mechanisms 
    for launching Spark-based applications on HPC clusters involve 
    batch-requesting of the resources which can greatly increase 
    queuing times of the Big Data applications. Pilot scheduling is a 
    well known strategy that decouples resource provisioning from task 
    scheduling, thereby allowing applications to leverage dynamic 
    pools of computing resources. Here, we introduce SPA, an 
    open-source
    pilot-job submission system designed for launching Apache Spark pipelines 
    on Slurm-based clusters. SPA leverages optimizations available in Spark, 
    such as robust workers, masters and driver, in addition to 
    checkpointing, to ensure robustness against nodes lost due to
    expired walltime. SPA also ensures that a 
    user-specified number of pilots is maintained in the cluster in 
    order to ensure a constant level of parallelism. Should pilots be 
    idle for a certain amount of time, they are terminated by 
    the system. We evaluate SPA on neuroimaging pipelines, our main domain of interest. \tristan{Add a sentence on results when you have them.}

\end{abstract}

\section{Introduction}

In the recent years, neuroimaging Big Data has become more accessible, as a
result of the growing number of open-data initiatives~\cite{openneuro, hcp, ukbiobank}. However, the
processing of such large datasets using standard neuroimaging pipelines has 
become cumbersome. As a result, many researchers are moving towards Big Data 
engines, such as Apache Spark~\cite{spark} and Dask~\cite{sdask}, to 
create their processing pipelines. Unlike traditional neuroimaging 
engines, Big Data engines were designed to be executed on dedicated 
commodity infrastructures using Big Data schedulers. However, 
researchers still largely rely on High Performance Computing (HPC) 
infrastructure to process their data, although they may have access to 
clouds or local workstations.

High-Performance Computing infrastructure differs from the commodity infrastructure 
typically used by Big Data platforms in that it is not dedicated. That is, there 
are multiple users running a variety of different applications and all using the 
same resources. Schedulers used on HPC infrastructure include PBS, SGE, Slurm, HTCondor and 
TORQUE~\cite{schedulers}. The majority of these are batch submission schedulers
in which the number of resources are requested by the user and the specified 
program is executed on the infrastructure.

Another popular HPC scheduling approach,
implemented notably by HTCondor~\cite{htcondor}, is known as pilot scheduling. Rather than requesting
all necessary resources in a single batch call, a pilot-scheduling system will request
multiple instances of subsets of the required resources. Processing starts 
as soon as a minimal amount of resources are allocated, and can be replenished if 
resources are lost. Such a scheduling approach decreases the time a 
user spends in the queue waiting for resources as less resources are 
being requested per instance.\todo{need citation} In addition, pilot 
scheduling makes it easier for users to estimate the number of 
cores required by their application, as a slight overestimation would not impact queuing 
time. Many scientific workflow engines are compatible with 
pilot-scheduling schedulers~\cite{nipype and others} and some others 
exclusively use pilot-scheduling approaches~\cite{Pegasus and PSOM}. 
\tristan{I'm not sure if Pegasus is exclusively pilot based.}

Apache Spark is a popular Big Data framework, commonly used in both industrial
and academic settings. Although it is a Scala-based framework, it also has APIs
for Java, Python (PySpark) and R. Spark's Resilient Distributed Dataset (RDD) abstraction enabled
in-memory processing of pipelines by co-locating tasks and data, which provided important performance improvements compared to its predecessors.
 Through the use of RDDs, it also became possible to execute iterative 
workflows -- something not easily doable in older frameworks such as Hadoop MapReduce.
Schedulers for Spark include its built-in standalone schedule, Yet Another Resource Negotiator (YARN),
and Mesos~\cite{yarn, mesos}. Out of the three schedulers, only Mesos can be used
as an HPC scheduler, which is not frequently done. \tristan{why?}


Projects such as Thunder~\cite{thunder}, a PySpark-compatible library 
of image processing tools, have been developed in various fields 
although they are still not widely adopted, possibly due to the 
difficulties in adapting Big Data frameworks on HPC \todo{better 
explain the difficulties...setting walltimes etc. also explain 
neuroimaging 
applications}. In addition, Spark has been used in 
various research projects to process neuroimaging data \cite{Boubela, 
ariel's paper, maybe the simulation one}. \tristan{in this paragraph you could refer
to your pre-print for more information.}

The recommended method for launching Spark applications on HPC schedulers 
involves batch requesting all the necessary resources and launching a standalone
Spark cluster once the resources have been requested. This could significantly increase
the scheduling time of Spark-based applications as a large amount of resources 
may be necessary to process Big Data. In a pilot-scheduling model, rather than requesting all the resources 
at once, a Spark cluster is launched with a subset of the resources and is expanded as 
more resources get allocated. This, in turn, may reduce the overall processing of
an application as less resources are requested at once and are therefore more likely
to be scheduled faster.

There have been some efforts to use pilot-scheduling approaches with Spark applications
running on HPC \cite{jha and spark on pbs paper}. However, these projects are either
not open-source or not necessarily compatible with Slurm.

Here we present SPA, a pilot-scheduling library for launching Spark applications 
on HPC infrastructures. This library currently enables the launching of Spark applications
on Slurm-based clusters by dividing the total amount of resources required and requesting
multiple instances of subsets of the resources at a time. When a single 
instance is started, a Standalone Spark cluster is started. Built-in 
fault-tolerance of Spark masters and workers is leveraged, in addition 
to Driver fault tolerance for Scala-based applications. When nodes are 
lost due to walltime expiration, additional pilot nodes are launched 
until the driver completes. SPA facilitates the execution of large 
neuroimaging Spark applications on HPC systems.


 
\section{Materials and Methods}\label{sec:methods}
    \todo{walltime parameter of workflow}
    \subsection{Added value of pilot scheduling}
        \todo{execution of a batch cluster vs dynamic (pilot) cluster}
    \subsection{Robust masters}
        \todo{scala standalone vs our pyspark workaround. kill masters in experiments}
    \subsection{Checkpointing}
        \todo{metric for determining how often to checkpoint based on cluster size}
    \subsection{Job arrays}
        \todo{need to kill idle workers. may not want all workers to be running at once.}
    \subsection{Example application}
        \todo{incrementation with varying task durations}
\section{Discussion}\label{sec:discussion}
\section{Conclusion}\label{sec:conclusion}

\end{document}
