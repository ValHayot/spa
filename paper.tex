\documentclass{IEEEtran}
\usepackage[hyphens]{url}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{colortbl} % for \rowcolor
\usepackage{caption}
\usepackage{subcaption}
\usepackage[bookmarks=false]{hyperref}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=blue,filecolor=black,urlcolor=blue}

\newcommand{\tristan}[1]{\color{red}\textbf{Note from Tristan}:
      #1 \color{black}}
\newcommand{\TG}[1]{\tristan{#1}}

\newcommand{\todo}[1]{\marginpar{\parbox{18mm}{\flushleft\tiny\color{red}\textbf{TODO}:
      #1}}}
\definecolor{headcolor}{gray}{0.9}

\begin{document}
\title{Evaluation of dynamic resource allocation strategies for overlay Apache
       Spark clusters on HPC}
\author{
    \IEEEauthorblockN{
        Val\'erie Hayot-Sasson and Tristan Glatard
    }
    \IEEEauthorblockA{}
}
\maketitle

\begin{abstract}
    Big Data has become prominent throughout many scientific fields and, as a
    result, scientific communities have sought out Big Data frameworks to 
    accelerate the processing of their increasingly data-intensive pipelines.
    \TG{I'd write the previous sentence in continuous present tense, since I think
    this is an on-going process.}
    However, while scientific communities typically rely on High-Performance 
    Computing (HPC) clusters for the parallelization of their pipelines, many 
    popular Big Data frameworks such as Hadoop and Spark were primarily designed
    to be executed on dedicated commodity infrastructures. Differences found between
    these infrastructures and the policies that surround them limit the advantages
    of deploying Big Data pipelines on HPC infrastructure. For instance, HPC
    clusters typically employ a batch job submission system requiring users to
    specify details on number of cores, nodes, amount of memory and duration.
    Big Data engines, such as Spark, have no built-in mechanism to interact with
    HPC batch job submission systems. Morever, the details required by the HPC
    schedulers \TG{be more specific than ``details'': do you mean ``scheduling policies'',
    ``application resource requirerments?''
    } are not necessarily initially known to users of Big Data frameworks.
    \TG{The previous paragraph is a bit long: it could be summarized to 2-3
    sentences.}

    Pilot scheduling strategies have been developed to address the limitations 
    of traditional HPC batch job schedulers \TG{You could give examples of pilot schedulers as you did with Spark and Hadoop}.
    Pilot schedulers decouple resource
    provisioning from task scheduling, thereby enabling efficient resource
    utilization through dynamic scheduling. This paper evaluates the benefits 
    pilot-scheduling strategies over traditional batch submission
    on HPC clusters with overlay Apache Spark clusters. We first evaluate the
    overall speedup brought on by employing pilot-scheduling strategies. We then
    examine the robustness to master and driver failures, which may be frequent 
    when underestimating pilot walltime. Spark built-in checkpointing is also 
    investigated in relation to pilot expiration.

    Our results show that\ldots TBD.
\end{abstract}

\section{Introduction}

With the increasing volume of data being collected and shared, Big Data has become
an important area of focus to many scientific domains. As is the case with neuroimaging,
dataset available publicly now reach up to Petabytes in size~\cite{UKBiobank, hcp}. Average workstations found in research lab would not be
able to support the processing of such large dataset, leaving researchers to look 
into cloud or High-Performance Computing~(HPC) solutions. Whereas both options are
suitable for the processing of scientific Big Data, HPC clusters remain the more 
sought after solution, potentially due to its cost-effectiveness.

Scientific workflows may be quite compute intensive, requiring more than a few
hours to complete (\cite{freesurfer, fmriprep, a paper which mentions duration}).
The coupling of large datasets with compute-intensive workflows may result in 
unreasonably long processing times. Such limitations restrict the amount data 
utilized in scientific experiments. To reduce the effects of Big Data on processing
times, researchers have begun to move towards employing Big Data frameworks for their
analyses. Examples of Big Data frameworks including Hadoop MapReduce~\cite{Hadoop},
Apache Spark~\cite{Spark} and Dask~\cite{Dask}. However, these frameworks were 
designed with dedicated commodity infrastructure in mind, and, with the exception
of Dask, do not support traditional HPC schedulers.

Within High-Performance Computing clusters, many users processing a variety of different
applications, share resources. These shared resources include storage (e.g. Parallel file system) and 
compute node resources (memory, cpu, local disk space, etc.). In order for users to 
gain access to shared compute resources, HPC clusters typically employ schedulers for 
batch resource provisioning. Examples of such schedulers include PBS, SGE, Slurm and 
TORQUE~\cite{schedulers}. These schedulers require that the user specify the amount
of resources required and the duration that they require the resources for. The users 
are then placed in a queue at a position based on the user's priority and resource 
availability.

There are several limitations to running applications on batch HPC schedulers, 
particularly in the face of Big Data. For instance, users may not necessarily 
know the exact requirements of the applications. With applications that are 
both data- and compute-intensive, this may result in the user overestimating the
resources required and the duration that these resources are required for. Frequent
high overestimates may create bottlenecks in the system, preventing other users
from occupying allocated idle resources. In contrast, users may underestimate required 
resources. This may subsequently result in many failures within the application leading to
additional re-executions of the same application. This is problematic for the user
as their priority may be affected by the cluster usage. Additionally, the user may 
require more time on the cluster to relaunch non-checkpointed and interrupted 
tasks. Furthermore, Big Data frameworks, such as Apache Spark, do not interact naturally
with HPC schedulers and typically rely on other schedulers for resource provisioning
and task scheduling. Therefore, to run Big Data applications on HPC schedulers, 
it is also necessary to start an overlay cluster.



In contrast to static resource provisioning, as is the case with batch HPC schedulers, 
dynamic resource provisioning, known as pilot job or glide-in scheduling, exists. 
This dynamic resource provisioning allows the user to specify a minimum and maximum 
amount of resources, and the cluster can expand and contract depending on application requirements.
Each pilot may consist of the minimal amount of resources required to launch the 
application. As each pilot would request allocation of fewer resources than that of a batch
request, Pilot Jobs would typically spend less time on the resource allocation 
queue. Furthermore, although individual pilots may expire due to walltimes, the dynamic
nature of pilots allows the entire application to surpass walltime limitations, as
additional pilots may be added to the pilot queue until application completion. 
Examples of HPC schedulers employing pilot strategies include HTCondor~\cite{htcondor}
and DIRAC~\cite{DIRAC}. \todo{maybe mention PSOM and Pegasus here}

Apache Spark is a popular Big Data framework, commonly used in both industrial
and academic settings. Although it is a Scala-based framework, it also has APIs
for Java, Python (PySpark) and R. Spark's Resilient Distributed Dataset (RDD) abstraction enabled
in-memory processing of pipelines by co-locating tasks and data, which provided important performance improvements compared to its predecessors.
 Through the use of RDDs, it also became possible to execute iterative 
workflows -- something not easily doable in older frameworks such as Hadoop MapReduce.
Schedulers for Spark include its built-in standalone schedule, Yet Another Resource Negotiator (YARN),
and Mesos~\cite{yarn, mesos}.


Projects such as Thunder~\cite{thunder}, a PySpark-compatible library 
of image processing tools, have been developed in various fields 
although they are still not widely adopted, possibly due to the 
difficulties in adapting Big Data frameworks on HPC \todo{better 
explain the difficulties...setting walltimes etc. also explain 
neuroimaging 
applications}. In addition, Spark has been used in 
various research projects to process neuroimaging data \cite{Boubela, 
ariel's paper, maybe the simulation one}. \tristan{in this paragraph you could refer
to your pre-print for more information.}

The recommended method for launching Spark applications on HPC schedulers 
involves batch requesting all the necessary resources and launching a standalone
Spark cluster once the resources have been requested \TG{Add link to Compute Canada's instructions?}. This could significantly increase
the scheduling time of Spark-based applications as a large amount of resources 
may be necessary to process Big Data. In a pilot-scheduling model, rather than requesting all the resources 
at once, a Spark cluster is launched with a subset of the resources and is expanded as 
more resources get allocated. This, in turn, may reduce the overall processing of
an application as less resources are requested at once and are therefore more likely
to be scheduled faster.

Pilot scheduling techniques have already been applied to scientific workflow engines, such as
Pegasus, PSOM and RADICAL-Pilot~\cite{}. While there have been some efforts on
implementing pilot scheduling approaches for Apache Spark\cite{jha and spark on pbs paper},
research is limited and none of the them detail the effects of an overlay cluster
on overall performance and robustness. 

In this paper, we will analyse the effects of pilot scheduling strategies with an 
overlay Spark cluster. Our evaluation will be broken down into two parts: 1)
the added value of pilots, and 2), tolerance to failures. For determining the 
added value of pilots, we will investigate the effect of queuing times on 
pilot applications as compared to a batch requests. As much of the application's
tolerance to failures are features brought on by the overlay Spark cluster, in
Part 2 we will introduce failures that would be typical to pilot scheduling 
approaches (e.g. node failures due to walltime expiration). This part will be 
broken down into master failures, driver failures and checkpointing. As pilots 
may need to be replenished during execution, we will also consider the effects of 
job queues \todo{reword}.

\TG{The introduction is still very much geared toward the presentation of 
the SPA software. Instead, I think it should introduce the question whether pilot jobs
are useful at all to overlay schedulers, explain why they may be, and why they may not (there is already an overlay scheduler,
and resource heterogeneity is limited). You could explain that the traditional benefits of pilot-job systems are:
\begin{itemize}
    \item Address worker node heterogeneity
    \item Address worker node failures (software, incl walltime expirations, or hardware)
    \item Address variable queuing times
    \item Tolerant to resource requirement overestimation, in particular walltime and CPU
    \item Increase workload distribution (use more nodes)
\end{itemize}
Among these, (1) and to some extent (2) are already addressed by an overlay scheduler.

The title should also reflect this.
}

\section{Materials and Methods}\label{sec:methods}
    SPA consists of a Python-based driver with bash-templates that are called 
    using the Slurmpy~\cite{slurmpy} library. In order to commence pilot-scheduling,
    the user must supply the bash template of choice and a JSON configuration 
    file which contains details on pilot node requirements and the desired application
    to be launched. Currently, there only exists a single pilot template for launching
    Spark jobs on a Slurm-based HPC cluster. The pseudocode for that template can 
    be seen in ~\ref{Figure:pilot-temp}.
    \todo{walltime parameter of workflow}
    \subsection{Added value of pilot scheduling}
        \todo{execution of a batch cluster vs dynamic (pilot) cluster}
    \subsection{Robust masters}
        \todo{scala standalone vs our pyspark workaround. kill masters in experiments}
    \subsection{Checkpointing}
        \todo{metric for determining how often to checkpoint based on cluster size}
    \subsection{Job arrays}
        \todo{need to kill idle workers. may not want all workers to be running at once.}
    \subsection{Example application}
        \todo{incrementation with varying task durations}
\section{Discussion}\label{sec:discussion}
\section{Conclusion}\label{sec:conclusion}

\end{document}
