\documentclass{IEEEtran}
\usepackage[hyphens]{url}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{colortbl} % for \rowcolor
\usepackage{caption}
\usepackage{subcaption}
\usepackage[bookmarks=false]{hyperref}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=blue,filecolor=black,urlcolor=blue}

\newcommand{\tristan}[1]{\color{red}\textbf{Note from Tristan}:
      #1 \color{black}}
\newcommand{\TG}[1]{\tristan{#1}}

\newcommand{\todo}[1]{\marginpar{\parbox{18mm}{\flushleft\tiny\color{red}\textbf{TODO}:
      #1}}}
\definecolor{headcolor}{gray}{0.9}

\begin{document}
\title{SPA: An Apache Spark Pilot scheduler\\ for HPC applications}
\author{
    \IEEEauthorblockN{
        Val\'erie Hayot-Sasson and Tristan Glatard
    }
    \IEEEauthorblockA{}
}
\maketitle

\begin{abstract}
    Big Data has become prominent throughout many scientific fields and, as a
    result, scientific communities have sought out Big Data frameworks to 
    accelerate the processing of their increasingly data-intensive pipelines.
    \TG{I'd write the previous sentence in continuous present tense, since I think
    this is an on-going process.}
    However, while scientific communities typically rely on High-Performance 
    Computing (HPC) clusters for the parallelization of their pipelines, many 
    popular Big Data frameworks such as Hadoop and Spark were primarily designed
    to be executed on dedicated commodity infrastructures. Differences found between
    these infrastructures and the policies that surround them limit the advantages
    of deploying Big Data pipelines on HPC infrastructure. For instance, HPC
    clusters typically employ a batch job submission system requiring users to
    specify details on number of cores, nodes, amount of memory and duration.
    Big Data engines, such as Spark, have no built-in mechanism to interact with
    HPC batch job submission systems. Morever, the details required by the HPC
    schedulers \TG{be more specific than ``details'': do you mean ``scheduling policies'',
    ``application resource requirerments?''
    } are not necessarily initially known to users of Big Data frameworks.
    \TG{The previous paragraph is a bit long: it could be summarized to 2-3
    sentences.}

    Pilot scheduling strategies have been developed to address the limitations 
    of traditional HPC batch job schedulers \TG{You could give examples of pilot schedulers as you did with Spark and Hadoop}.
    Pilot schedulers decouple resource
    provisioning from task scheduling, thereby enabling efficient resource
    utilization through dynamic scheduling. This paper evaluates the benefits 
    pilot-scheduling strategies over traditional batch submission
    on HPC clusters with overlay Apache Spark clusters. We first evaluate the
    overall speedup brought on by employing pilot-scheduling strategies. We then
    examine the robustness to master and driver failures, which may be frequent 
    when underestimating pilot walltime. Spark built-in checkpointing is also 
    investigated in relation to pilot expiration.

    Our results show that\ldots TBD.
\end{abstract}

\section{Introduction}

In the recent years, neuroimaging \TG{If you want to put neuroimaging first, then you need to explain it. Just say that it's an example among many fields}
Big Data has become more accessible, as a
result of the growing number of open-data initiatives~\cite{openneuro, hcp, ukbiobank}. However, the
processing of such large datasets using standard neuroimaging pipelines has 
become cumbersome. As a result, many researchers are moving towards Big Data 
engines, such as Apache Spark~\cite{spark} and Dask~\cite{sdask}, to 
create their processing pipelines. Unlike traditional neuroimaging 
engines, Big Data engines were designed to be executed on dedicated 
commodity infrastructures using Big Data schedulers. However, 
researchers still largely rely on High Performance Computing (HPC) 
infrastructure to process their data, although they may have access to 
clouds or local workstations.

High-Performance Computing infrastructure differs from the commodity infrastructure 
typically used by Big Data platforms in that it is not dedicated. That is, there 
are multiple users running a variety of different applications and all using the 
same resources. Schedulers used on HPC infrastructure include PBS, SGE, Slurm, HTCondor and 
TORQUE~\cite{schedulers}. The majority of these are batch submission schedulers
in which the number of resources are requested by the user and the specified 
program is executed on the infrastructure.

Another popular HPC scheduling approach,
implemented notably by HTCondor~\cite{htcondor}, is known as pilot scheduling. Rather than requesting
all necessary resources in a single batch call, a pilot-scheduling system will request
multiple instances of subsets of the required resources. Processing starts 
as soon as a minimal amount of resources are allocated, and can be replenished if 
resources are lost. Such a scheduling approach decreases the time a 
user spends in the queue waiting for resources as less resources are 
being requested per instance.\todo{need citation} In addition, pilot 
scheduling makes it easier for users to estimate the number of 
cores required by their application, as a slight overestimation would not impact queuing 
time. Many scientific workflow engines are compatible with 
pilot-scheduling schedulers~\cite{nipype and others} and some others 
exclusively use pilot-scheduling approaches~\cite{Pegasus and PSOM}. 
\tristan{I'm not sure if Pegasus is exclusively pilot based.}

Apache Spark is a popular Big Data framework, commonly used in both industrial
and academic settings. Although it is a Scala-based framework, it also has APIs
for Java, Python (PySpark) and R. Spark's Resilient Distributed Dataset (RDD) abstraction enabled
in-memory processing of pipelines by co-locating tasks and data, which provided important performance improvements compared to its predecessors.
 Through the use of RDDs, it also became possible to execute iterative 
workflows -- something not easily doable in older frameworks such as Hadoop MapReduce.
Schedulers for Spark include its built-in standalone schedule, Yet Another Resource Negotiator (YARN),
and Mesos~\cite{yarn, mesos}. Out of the three schedulers, only Mesos can be used
as an HPC scheduler, which is not frequently done. \tristan{why?}


Projects such as Thunder~\cite{thunder}, a PySpark-compatible library 
of image processing tools, have been developed in various fields 
although they are still not widely adopted, possibly due to the 
difficulties in adapting Big Data frameworks on HPC \todo{better 
explain the difficulties...setting walltimes etc. also explain 
neuroimaging 
applications}. In addition, Spark has been used in 
various research projects to process neuroimaging data \cite{Boubela, 
ariel's paper, maybe the simulation one}. \tristan{in this paragraph you could refer
to your pre-print for more information.}

The recommended method for launching Spark applications on HPC schedulers 
involves batch requesting all the necessary resources and launching a standalone
Spark cluster once the resources have been requested \TG{Add link to Compute Canada's instructions?}. This could significantly increase
the scheduling time of Spark-based applications as a large amount of resources 
may be necessary to process Big Data. In a pilot-scheduling model, rather than requesting all the resources 
at once, a Spark cluster is launched with a subset of the resources and is expanded as 
more resources get allocated. This, in turn, may reduce the overall processing of
an application as less resources are requested at once and are therefore more likely
to be scheduled faster.

There have been some efforts to use pilot-scheduling approaches with Spark applications
running on HPC \cite{jha and spark on pbs paper}. However, these projects are either
not open-source or not necessarily compatible with Slurm. 

Here we present SPA, a pilot-scheduling library for launching Spark applications 
on HPC infrastructures. This library currently enables the launching of Spark applications
on Slurm-based clusters by dividing the total amount of resources required and requesting
multiple instances of subsets of the resources at a time. When a single 
instance is started, a Standalone Spark cluster is started. Built-in 
fault-tolerance of Spark masters and workers is leveraged, in addition 
to Driver fault tolerance for Scala-based applications. When nodes are 
lost due to walltime expiration, additional pilot nodes are launched 
until the driver completes. SPA facilitates the execution of large 
neuroimaging Spark applications on HPC systems.

\TG{The introduction is still very much geared toward the presentation of 
the SPA software. Instead, I think it should introduce the question whether pilot jobs
are useful at all to overlay schedulers, explain why they may be, and why they may not (there is already an overlay scheduler,
and resource heterogeneity is limited). You could explain that the traditional benefits of pilot-job systems are:
\begin{itemize}
    \item Address worker node heterogeneity
    \item Address worker node failures (software, incl walltime expirations, or hardware)
    \item Address variable queuing times
    \item Tolerant to resource requirement overestimation, in particular walltime and CPU
    \item Increase workload distribution (use more nodes)
\end{itemize}
Among these, (1) and to some extent (2) are already addressed by an overlay scheduler.

The title should also reflect this.
}

\section{Materials and Methods}\label{sec:methods}
    SPA consists of a Python-based driver with bash-templates that are called 
    using the Slurmpy~\cite{slurmpy} library. In order to commence pilot-scheduling,
    the user must supply the bash template of choice and a JSON configuration 
    file which contains details on pilot node requirements and the desired application
    to be launched. Currently, there only exists a single pilot template for launching
    Spark jobs on a Slurm-based HPC cluster. The pseudocode for that template can 
    be seen in ~\ref{Figure:pilot-temp}.
    \todo{walltime parameter of workflow}
    \subsection{Added value of pilot scheduling}
        \todo{execution of a batch cluster vs dynamic (pilot) cluster}
    \subsection{Robust masters}
        \todo{scala standalone vs our pyspark workaround. kill masters in experiments}
    \subsection{Checkpointing}
        \todo{metric for determining how often to checkpoint based on cluster size}
    \subsection{Job arrays}
        \todo{need to kill idle workers. may not want all workers to be running at once.}
    \subsection{Example application}
        \todo{incrementation with varying task durations}
\section{Discussion}\label{sec:discussion}
\section{Conclusion}\label{sec:conclusion}

\end{document}
