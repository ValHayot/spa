\documentclass{IEEEtran}
\usepackage[hyphens]{url}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{colortbl} % for \rowcolor
\usepackage{ulem} % for \uwave
\usepackage{caption}
\usepackage{subcaption}
\usepackage[bookmarks=false]{hyperref}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=blue,filecolor=black,urlcolor=blue}
\usepackage{algorithm}
\usepackage{algorithmicx} % Doc is at http://tug.ctan.org/macros/latex/contrib/algorithmicx/algorithmicx.pdf
\usepackage{algpseudocode}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}
\algblock{Input}{EndInput}
\algnotext{EndInput}
\algblock{Output}{EndOutput}
\algnotext{EndOutput}
\newcommand{\Desc}[2]{\State \makebox[2em][l]{#1}#2}

\newcommand{\tristan}[1]{\color{red}\textbf{Note from Tristan}:
      #1 \color{black}}
\newcommand{\TG}[1]{\tristan{#1}}

\newcommand{\todo}[1]{\marginpar{\parbox{18mm}{\flushleft\tiny\color{red}\textbf{TODO}:
      #1}}}
\definecolor{headcolor}{gray}{0.9}

\begin{document}
\title{Evaluation of pilot jobs for Apache Spark\\ applications on HPC clusters}
\author{
    \IEEEauthorblockN{
        Val\'erie Hayot-Sasson and Tristan Glatard \\
        Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada
    }
    \IEEEauthorblockA{}
}
\maketitle

\begin{abstract}
    Big Data has become prominent throughout many scientific fields and, as
    a result, scientific communities have sought out Big Data frameworks to
    accelerate the processing of their increasingly data-intensive
    pipelines. However, while scientific communities typically rely on
    High-Performance Computing (HPC) clusters for the parallelization of
    their pipelines, many popular Big Data frameworks such as Hadoop and
    Apache Spark were primarily designed to be executed on dedicated
    commodity infrastructures. This paper evaluates the benefits of pilot
    jobs over traditional batch submission on HPC clusters with overlay
    Apache Spark clusters. We first evaluate the overall speedup brought on
    by employing pilot jobs. We then examine the robustness to master and
    driver failures, which may be frequent when underestimating pilot
    walltime. Spark built-in checkpointing is also investigated in relation
    to pilot expiration. Surprisingly, our results show that the speed-up
    provided by pilot jobs over batch scheduling remains moderate (\TG{X} on
    average) despite the presence of long queuing times. In addition, pilot
    jobs provide an extra layer of scheduling that complexifies debugging
    and deployment. We conclude that traditional batch scheduling should
    remain the default strategies to deploy Apache Spark applications on
    HPC clusters.
\end{abstract}

\section{Introduction}

Pilot jobs, also known as dynamic resource provisioning or glide-in
scheduling, are a widely-used technique to address infrastructure
heterogeneity, variable task queuing times, fine task granularity, and node
failures on distributed computing infrastructures~\cite{jakubsmodel}. Made popular by software
projects such as Condor~\cite{thain2005distributed} and DIRAC~\cite{casajus2010dirac}, they
became critical to grid computing, greatly improved the performance of HPC
clusters, and enabled multi-cloud executions. 

In contrast to static resource provisioning, pilot jobs are submitted to
the infrastructure separately from the application, to provision resources
on which application tasks will eventually be scheduled. Large pools of
resources can thus be created, shielding applications from the underlying
queuing times, failures, and other idiosyncracies. A variety of frameworks
now rely on pilot jobs, including RADICAL-Pilot~\cite{merzky2015radical},
and recent versions of the Pipeline System for Octave and Matlab
(PSOM)~\cite{bellec2012pipeline}. The survey in~\cite{turilli2018comprehensive} reviews
the current pilot-job systems.

% Goal of the paper
In this paper, we study the use of pilot jobs to deploy Apache
Spark~\cite{zaharia2016apache} applications on shared HPC clusters. The method
currently recommended for this purpose involves batch requesting all the
necessary resources and launching a standalone Spark cluster once the
resources have been allocated. This is for instance the method used in
Compute Canada (see details \href{https://docs.computecanada.ca/wiki/Apache_Spark/en}{here}),
 our national computing infrastructure. With pilot
jobs, rather than requesting all the resources at once, a Spark cluster is
launched with a subset of the resources and is expanded as more resources
get allocated. Consistently with previous examples of pilot job
deployments, we hypothesize that this strategy would reduce queuing times
by (1) fragmenting resource requirements, and (2) allowing shorter walltime
estimates. While there have been some efforts on implementing pilot jobs
for Apache Spark~\cite{jha-spark-pbs}, research is limited and
none of the them detail their quantitative effect.

Apache Spark is a popular Big Data framework, commonly used in both
industrial and academic settings. Although it is a Scala-based framework,
it also has APIs for Java, Python (PySpark) and R. Spark's Resilient
Distributed Dataset (RDD) abstraction enabled in-memory processing of
pipelines by co-locating tasks and data, which provided important
performance improvements compared to its predecessor Hadoop
MapReduce~\cite{dean2008mapreduce}. Through the use of RDDs, it also became possible
to execute iterative workflows -- something not easily doable in older
frameworks. Schedulers for Spark include its built-in standalone schedule,
Yet Another Resource Negotiator (YARN~\cite{apache13yet}), and Mesos~\cite{hindman2011mesos}. As a
result of the sustained increase in data volumes, Apache Spark and other
Big Data engines are increasingly used for scientific applications,
including in neuroimaging, our primary field of
interest~\cite{boubela2016big,mehta2017comparative,maybethesimulationone,freeman2014mapping}.


Big Data frameworks were designed with dedicated commodity infrastructure
in mind, and, with the exception of Dask~\cite{rocklin2015dask}, do not support batch
HPC schedulers such as PBS, SGE, Slurm and TORQUE~\cite{schedulers}, which
are commonly available to scientists. Therefore, to run Big Data
applications on HPC schedulers, it is also necessary to start an overlay
cluster that will schedule application tasks on resources provisioned
through batch schedulers. Our experiments quantify the effect of pilot jobs
when combined with such an overlay cluster. Our evaluation will be broken
down into two parts:
1) the added value of pilots, and 2), tolerance to failures.

To summarize, our paper
makes the following contributions:
\begin{itemize}
\item We present a lightweight pilot-job framework to run Apache Spark
applications on HPC clusters.
\item We compare the makespan of a typical neuroimaging Big Data
application with and without pilot jobs on two different HPC clusters and
in different conditions.
\item We describe a simple performance model to validate that the observed
differences come from variations in queuing times rather than other
factors.
\end{itemize}
Our methods, including infrastructure, job templates, application and
performance model are described in Section~\ref{sec:methods}.
Section~\ref{sec:results} presents our results which are discussed in
Section~\ref{sec:discussion}. Section~\ref{sec:conclusion} concludes on the 
relevant of pilot jobs for Apache Spark applications on HPC clusters.

\section{Materials and Methods}\label{sec:methods}

    The application, templates, configuration files, benchmarks and
    analysis scripts are publicly available and can be found in our Spark
    Pilot-job scheduler for HPC Applications (SPA) software at:
    \href{https://github.com/big-data-lab-team/spa}{https://github.com/big-data-lab-team/spa}.
    Links to the processing engines and processed data are provided in the
    text.
    
    \subsection{Infrastructure}
    All experiments were conducted on the Cedar and B\'eluga HPC computing clusters
    made available by Compute Canada through WestGrid and Calcul Qu\'ebec \TG{add links}. Both 
    clusters are accessible through the Slurm batch scheduler and Lustre parallel file system.
    The Cedar cluster has a total of 1542 nodes
    with a total of 58,416 CPU cores. Available memory on a Cedar node can range
    from 125 to 3022~G. Standard nodes are equipped with either 2x Intel E5-2683 v4
    Broadwell @ 2.1~Ghz (32 cores total) or 2x Intel Platinum 8160F Skylake @
    2.1Ghz (48 cores total) CPUs and 
    2 x 480~G SSD. All nodes and temporary storage on Cedar are connected by an 
    Intel OmniPath (version 1) with 100Gbit/s bandwidth.

    B\'eluga, on the other hand, is a smaller cluster with 872 available nodes. 
    Node memory can range between 92 to 752~G, with the most common node type having 
    186G. All nodes contain 2 x Intel Gold 6148
    Skylake @ 2.4~Ghz (40 cores/node) CPU and are connected to each other with a
    100~Gb/s Mellanox Infiniband EDR network \TG{expand EDR}. Each non-GPU node type contains one 
    480~G SSD. 

    \subsection{Job Templates}

    Two different job templates were used to implement the two main
    conditions compared in our experiments: the batch submission
    template and the pilot submission template. The batch submission template was
    inspired by the template provided by Compute Canada to launch Spark applications 
    on Slurm \href{https://docs.computecanada.ca/wiki/Apache\_Spark/en}{available here}.
    The template operates as follows: Certain resource requirements are requested by
    the user (e.g. walltime, amount of memory per node, number of CPUs per task, number 
    of nodes and number of tasks per node). Once these resources are allocated, a 
    master is started on one of the requested node resources. Then, after the master
    has successfully started, the workers are started on all nodes. Multiple worker
    instances are started on a single node by setting the \texttt{SPARK\_WORKER\_INSTANCES}
    environment variable to the number of tasks per node. Each worker is given as many
     cores as specified by the user in the Slurm resource allocation request.
    After both the masters and the workers have successfully started, the driver is finally
    started. The amount of memory given to each executor corresponds to 95\% of the 
    available memory on the node, to allow for offheap space. 
    The Spark deploy-mode selected for the batch template is client-mode. 

    The pilot submission template is similar to that of the batch template, however,
    each pilot will start its own Spark master and worker. Only one pilot,
    the first pilot to attempt to launch the driver, will do so. The reason for 
    which each pilot starts its own master is to ensure the fault tolerance of the 
    masters. In this sort of configuration, should the active master be killed, one
    of the stand-by masters will takeover and the application will be able to 
    resume. Such a configuration is particularly favourable in pilot scheduling 
    scenarios as node failures may be more frequent due to walltime expiration.
    Additionally, the Spark deploy mode of the driver was selected to be cluster deploy mode.
    This would not only allow the driver to be executed directly on one of the workers,
    but also allow us to make the driver fault-tolerant through the
    \texttt{supervise} mode, which is only available in cluster deploy
    \TG{A brief section on Spark to explain these modes and features would
    be useful}. As with the masters, it is particularly important to have a
    fault-tolerant driver in pilot-scheduling scenarios due to possible
    walltime expiration \TG{It should be explained before that one of the interests of pilots
    is to ``play'' with shorter walltimes}. \TG{It would also be nice to have runs where pilots 
    have shorter walltimes, to see if queuing times are shorter too.}
    Should pilots be idle for a certain duration, the
    pilots will terminate themselves such as to not hog resources.
    
    Due to the differences in deploy modes between batch and pilot submission, 
    batch will always inevitably have one more worker than pilot. This is because
    in cluster deploy, which pilot uses, the driver occupies a worker, whereas in
    client deploy, the driver is separate from any worker.

    Both of these Slurm templates are launched within a Python application called
    \textit{SPA}. The templates are used in conjunction with JSON configuration 
    file and passed to the SlurmPy~\cite{slurmpy} library within \textit{SPA}.
    The \textit{SPA} application, all the while ensures that all is preconfigured
    correctly before passing it to SlurmPy. It also ensures that enough pilots are
    launched, maintains track of the running/queued pilots, and launches additional
    pilots if there are less pilots than requested by the user in the Slurm queue.



    \subsection{Application}
        \begin{algorithm}\caption{Incrementation}\label{alg:incrementation}              
            \begin{algorithmic}[1]                                                       
                \Input                                                                       
                    \Desc{$x$}{a sleep delay in seconds}                                         
                    \Desc{$n$}{a number of iterations}                                           
                    \Desc{$C$}{a set of image chunks}                                            
                \EndInput                                                                    
                \ForEach{$chunk \in C$}                                                      
                    \State read $chunk$                                        
                    \For{$i \in [1, n]$}                                                         
                        \State $chunk\gets chunk+1$                                              
                        \State sleep $x$ \TG{Don't you sleep by x minus the duration of the previous step?}                                                        
                    \EndFor                                                                      
                    \State write $chunk$                                            
                \EndFor                                                                      
            \end{algorithmic}                                                                
        \end{algorithm}
    To determine the added value of pilot scheduling over batch scheduling of Spark
    applications, we required a Spark application operating on a large dataset with
    an important processing time to emulate a what would be the average requirements
    of a scientific Spark application. For this, we created a synthetic application 
    that would process the Big Brain~\cite{amunts2013bigbrain}, a 76~G 3D histological
    image of a human brain. The algorithm is a chain of map transformations that
    at each transformation increment the voxels of the image by 1 (see Algorithm~\ref{alg:incrementation}).
    We chose such a synthetic algorithm
    as the focus of our experiments is pilot scheduling and not the application in
    itself. Furthermore, this algorithm enabled us to have control over the task duration
    which was representative of scientific applications. Additionally, it was important
    that the overall application duration did not vary between the different levels of
    parallelism within our experiments. Being able to adjust the task duration based on
    level of parallelism allowed us to achieve this.

    Investigating the fault-tolerance provided by the overlay cluster is
    crucial in determining the suitability of pilot scheduling with overlay clusters.
    Fault-tolerance of the driver is only possible in cluster deploy mode, however,
    when using Spark's standalone scheduler, this mode is not available for Python 
    applications. It is for this reason that our synthetic application is written
    in Scala. Nevertheless, cluster deploy mode is possible for Python application
    using YARN or Mesos schedulers.

    
   
    \todo{walltime parameter of workflow}
    \subsection{Model}

    The makespan of an application can be defined as the total duration
    between the submission time of the first application task, and the
    completion time of the last application time. It includes any
    scheduling time, queueing time, data transfer time, and any other
    overhead.
    
    Assuming a divisible load, i.e., the application can be divided in any
    number of tasks, the makespan can be expressed using the following
    expression, which holds for both batch and pilot execution modes:
    \begin{equation}
        M = \frac{C}{W} \label{eq:mcw}
    \end{equation}
    where:
    \begin{itemize}
        \item $M$ is the makespan of the application
        \item $C$ is the total CPU time of the application
        \item $W$ is the average number of Spark workers throughout the execution
    \end{itemize}
    The average number of workers $W$ allows us to obtain a more accurate makespan
    figure as the average number of workers will decrease with increasing 
    queuing times. It is computed as follows:
    $$
    W = \frac{1}{M}\int_0^M{w(t)dt},
    $$
    where $w(t)$ is the number of workers available at time $t$. When the
    application is not subject to any scheduling or queuing time, the
    average number of workers equal the number of workers requested. 

    Therefore, assuming a fixed total CPU time, the relation
    between batch and pilot jobs can be represented as:
    \begin{equation}
        \frac{M_{batch}}{M_{pilot}} = \frac{W_{pilot}}{W_{batch}}\label{eq:makespancomp}
    \end{equation}
    where:
    \begin{itemize}
        \item $M_{batch}$ is the makespan of the batch application
        \item $M_{pilot}$ is the makespan of the pilot application
        \item $W_{pilot}$ is the average number of workers of the pilot application
        \item $W_{batch}$ is the average number of workers of the batch application
    \end{itemize}
    We will use this relation to discuss our results later on. It
    corresponds to an ideal case where no data or other overhead is
    present: only queuing times are included.

    \todo{perhaps expand a bit here \TG{I have a formal proof for eq (1) but I don't think
    it's required here.}}

    \subsection{Added value of pilot scheduling}
        \begin{table}                                                                    
            \centering                                                                       
            \begin{tabular}{c|c|c}                                                             
            \rowcolor{headcolor}                                                             
            Configuration & RAM (GB) & Tasks \\                               
            \hline                                                                           
            1 & 112 & 16 \\                                               
            2 & 224 & 32 \\                                               
            3 & 336 & 48 \\
            4 & 448 & 64 \\
            \end{tabular}                                                                    
            \setlength{\belowcaptionskip}{-10pt}                                             
            \caption{Resource configurations}                                                    
            \label{table:dedicatednodes}                                                            
        \end{table} 
           
        \begin{table*}                                                                   
        \centering                                                                       
        \begin{tabular}{c|cccccc}                                                   
          \rowcolor{headcolor}                                                           
          \multicolumn{7}{c}{Configuration 1}\\                      
          \hline                                                                         
          \rowcolor{headcolor}                                                           
          execution mode & nodes/job & RAM (G) & CPUs per task & tasks/node & walltime & task delay (s) \\                             
          \hline
          Batch & 1 & 112 & 1 & 16 & 2h30 & 45 \\
          8 pilots & 1 & 14 & 1 & 2 & 2h30 & 45 \\
          16 pilots & 1 & 7 & 1 & 1 & 2h30 & 45 \\

          \hline                                                                           
          \multicolumn{7}{c}{}\\                                                        

          \rowcolor{headcolor}                                                           
          \multicolumn{7}{c}{Configuration 2}\\                      
          \hline                                                                         
          \rowcolor{headcolor}                                                           
          execution mode & nodes/job & RAM (G) & CPUs per task & tasks/node & walltime & task delay (s) \\                             
          \hline
          Batch & 2 & 112 & 1 & 16 & 2h30 & 90 \\
          8 pilots & 1 & 28 & 1 & 4 & 2h30 & 90 \\
          16 pilots & 1 & 14 & 1 & 2 & 2h30 & 90 \\

          \hline                                                                           
          \multicolumn{7}{c}{}\\                                                        

          \rowcolor{headcolor}                                                           
          \multicolumn{7}{c}{Configuration 3}\\                      
          \hline                                                                         
          \rowcolor{headcolor}                                                           
          execution mode & nodes/job & RAM (G) & CPUs per task & tasks/node & walltime & task delay (s) \\                             
          \hline
          Batch & 3 & 112 & 1 & 16 & 2h30 & 120 \\
          8 pilots & 1 & 42 & 1 & 6 & 2h30 & 120 \\
          16 pilots & 1 & 21 & 1 & 3 & 2h30 & 120 \\

          \hline                                                                           
          \multicolumn{7}{c}{}\\                                                        

          \rowcolor{headcolor}                                                           
          \multicolumn{7}{c}{Configuration 4}\\                      
          \hline                                                                         
          \rowcolor{headcolor}                                                           
          execution mode & nodes/job & RAM (G) & CPUs per task & tasks/node & walltime & task delay (s) \\                             
          \hline
          Batch & 4 & 112 & 1 & 16 & 2h30 & 180 \\
          8 pilots & 1 & 56 & 1 & 8 & 2h30 & 180 \\
          16 pilots & 1 & 28 & 1 & 4 & 2h30 & 180 \\

          \hline                                                                           
          \multicolumn{7}{c}{}\\                                                        
        \end{tabular}                                                                    
        \setlength{\belowcaptionskip}{-10pt}                                             
        \caption{Experimental conditions}
        \label{table:conditions}                                                        
        \end{table*}                                                                                   
        To determine if there are any performance benefits to using pilot over 
        batch scheduling, we needed to compare both strategies given various resource
        requirements. It is expected that large batch requests will stay in the 
        resource queue longer than multiple pilot requests that ultimately use up
        the same amount of resources as each individual pilot requests less resources at
        a time. We therefore used four resource configurations to investigate this
        hypothesis (Table~\ref{table:dedicatednodes}).

       For batch, we requested 1 to 4 dedicated nodes, depending on resource 
       configuration, each with 112~G of RAM, 1 core per task and 16 tasks per 
       node. On the other hand, for all configurations, we ran our experiments
       with 8 and 16 pilots. Experimental conditions can be seen in Table~\ref{table:conditions}.

       As the focus of our experiments here is to measure the impact pilot scheduling
       has on queueing times, we wanted to ensure that our walltime estimates would
       be consistent for each experimental condition. Therefore, we adjusted task 
       delay based on the maximum level of parallelism to ensure that walltimes would
       not need to be readjusted for each configuration. Given utmost parallelism,
       the sleep delay added would amount to 1 hour of processing time regardless
       of configuration. The real processing time of the application, however, 
       would differ between configurations as we did not account for task duration
       in the sleep delay. However, task duration was not expected to signicantly impact
       overall processing time, but nevertheless, we set application walltime to 
       2 hours and 30 minutes for all configurations to account for additional
       processing time that may be incurred in applications with lower levels of 
       parallelism.

       For each configuration, all three execution modes were executed in parallel
       to ensure that the status of the overall cluster was the same when the 
       different execution modes were launched. Furthermore, the order of the execution modes
       for each configuration was randomized to ensure that SLURM job request
       order could not have affected results. 

       There were 15 repetitions in total for each configuration, and the order
       in which the configurations were launched was also randomized. This was to 
       account for any system variability that can occur, particularly in production
       HPC clusters. 

       Different clusters may have different resource configurations, number of 
       users and scheduling policies. Therefore, we executed all 15 repetitions on
       both Cedar and B\'eluga to determine how much our results differ between
       two distinct clusters.

    \subsection{Pipeline robustness}
        While nodes failures can occur for various reasons, underestimation of 
        walltime may lead to early node termination in pilot scheduling systems.
        Therefore, it is crucial for the overlay cluster within the pilot system
        to be robust to node failures. In this series of experiments, we investigate
        Spark's fault-tolerance with respect to worker, master and driver termination.

        In our configuration, the driver is running in cluster deploy-mode, resulting
        in it being executed directly on a worker. This feature alone should not make
        it fault-tolerant to failures. However, using this feature coupled with the 
        \texttt{--supervise} flag, will. 



        

        \todo{scala standalone vs our pyspark workaround. kill masters in experiments}
    \subsection{Checkpointing}
        \todo{metric for determining how often to checkpoint based on cluster size}
    \subsection{Job arrays}
        \todo{need to kill idle workers. may not want all workers to be running at once.}
    \subsection{Example application}
        \todo{incrementation with varying task durations}
\section{Results}\label{sec:results}
\section{Discussion}\label{sec:discussion}

SPA vs other frameworks.

Technical overhead of using pilots + an overlay cluster over a cluster.

\section{Conclusion}\label{sec:conclusion}

\section*{Acknowledgement}

We warmly thank Compute Canada and its regional centers WestGrid and Calcul
Qu\'ebec for providing the infrastructure used in our experiments.

\bibliographystyle{IEEEtran} 
\bibliography{biblio}
\end{document}
